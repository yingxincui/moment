#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡µé¢è®¡ç®—ç»“æœç¼“å­˜å·¥å…·æ¨¡å—
æ”¯æŒæŒ‰é¡µé¢å’Œå‚æ•°ç¼“å­˜è®¡ç®—ç»“æœï¼Œé»˜è®¤ç¼“å­˜ä¸€å¤©
"""

import streamlit as st
import json
import os
import hashlib
from datetime import datetime, timedelta
import pandas as pd
import pickle

# ç¼“å­˜é…ç½®
CACHE_DIR = "page_cache"
CACHE_DURATION_HOURS = 24  # ç¼“å­˜24å°æ—¶
CACHE_META_FILE = "cache_meta.json"

def ensure_cache_dir():
    """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)

def get_cache_key(page_name, params):
    """
    ç”Ÿæˆç¼“å­˜é”®
    
    Args:
        page_name: é¡µé¢åç§°
        params: å‚æ•°å­—å…¸
    
    Returns:
        cache_key: ç¼“å­˜é”®
    """
    # å°†å‚æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ’åºï¼Œç¡®ä¿ç›¸åŒå‚æ•°ç”Ÿæˆç›¸åŒé”®
    params_str = json.dumps(params, sort_keys=True, ensure_ascii=False)
    # ä½¿ç”¨é¡µé¢åç§°å’Œå‚æ•°ç”Ÿæˆå“ˆå¸Œé”®
    cache_key = f"{page_name}_{hashlib.md5(params_str.encode('utf-8')).hexdigest()}"
    return cache_key

def get_cache_file_path(cache_key):
    """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    ensure_cache_dir()
    return os.path.join(CACHE_DIR, f"{cache_key}.pkl")

def get_cache_meta_file_path():
    """è·å–ç¼“å­˜å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
    ensure_cache_dir()
    return os.path.join(CACHE_DIR, CACHE_META_FILE)

def load_cache_meta():
    """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
    meta_file = get_cache_meta_file_path()
    if os.path.exists(meta_file):
        try:
            with open(meta_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_cache_meta(meta_data):
    """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
    meta_file = get_cache_meta_file_path()
    try:
        with open(meta_file, 'w', encoding='utf-8') as f:
            json.dump(meta_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ä¿å­˜ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")

def is_cache_valid(cache_key):
    """
    æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    
    Args:
        cache_key: ç¼“å­˜é”®
    
    Returns:
        bool: ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    """
    meta_data = load_cache_meta()
    
    if cache_key not in meta_data:
        return False
    
    cache_info = meta_data[cache_key]
    cache_time = datetime.fromisoformat(cache_info['created_at'])
    
    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ç¼“å­˜æ—¶é—´
    if datetime.now() - cache_time > timedelta(hours=CACHE_DURATION_HOURS):
        return False
    
    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    cache_file = get_cache_file_path(cache_key)
    if not os.path.exists(cache_file):
        return False
    
    return True

def save_to_cache(cache_key, data, page_name, params):
    """
    ä¿å­˜æ•°æ®åˆ°ç¼“å­˜
    
    Args:
        cache_key: ç¼“å­˜é”®
        data: è¦ç¼“å­˜çš„æ•°æ®
        page_name: é¡µé¢åç§°
        params: å‚æ•°å­—å…¸
    """
    try:
        # ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
        cache_file = get_cache_file_path(cache_key)
        with open(cache_file, 'wb') as f:
            pickle.dump(data, f)
        
        # æ›´æ–°å…ƒæ•°æ®
        meta_data = load_cache_meta()
        meta_data[cache_key] = {
            'page_name': page_name,
            'params': params,
            'created_at': datetime.now().isoformat(),
            'data_type': type(data).__name__,
            'file_size': os.path.getsize(cache_file)
        }
        save_cache_meta(meta_data)
        
        print(f"ç¼“å­˜å·²ä¿å­˜: {page_name} - {cache_key}")
        
    except Exception as e:
        print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def load_from_cache(cache_key):
    """
    ä»ç¼“å­˜åŠ è½½æ•°æ®
    
    Args:
        cache_key: ç¼“å­˜é”®
    
    Returns:
        data: ç¼“å­˜çš„æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–æ— æ•ˆåˆ™è¿”å›None
    """
    try:
        cache_file = get_cache_file_path(cache_key)
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                data = pickle.load(f)
            print(f"ä»ç¼“å­˜åŠ è½½: {cache_key}")
            return data
    except Exception as e:
        print(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
    return None

def get_cached_result(page_name, params, compute_func, *args, **kwargs):
    """
    è·å–ç¼“å­˜ç»“æœï¼Œå¦‚æœç¼“å­˜æ— æ•ˆåˆ™é‡æ–°è®¡ç®—
    
    Args:
        page_name: é¡µé¢åç§°
        params: å‚æ•°å­—å…¸
        compute_func: è®¡ç®—å‡½æ•°
        *args: è®¡ç®—å‡½æ•°çš„å‚æ•°
        **kwargs: è®¡ç®—å‡½æ•°çš„å…³é”®å­—å‚æ•°
    
    Returns:
        result: è®¡ç®—ç»“æœ
    """
    cache_key = get_cache_key(page_name, params)
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    if is_cache_valid(cache_key):
        cached_data = load_from_cache(cache_key)
        if cached_data is not None:
            return cached_data
    
    # ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°è®¡ç®—
    print(f"ç¼“å­˜æ— æ•ˆï¼Œé‡æ–°è®¡ç®—: {page_name}")
    result = compute_func(*args, **kwargs)
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if result is not None:
        save_to_cache(cache_key, result, page_name, params)
    
    return result

def clear_page_cache(page_name=None):
    """
    æ¸…é™¤é¡µé¢ç¼“å­˜
    
    Args:
        page_name: é¡µé¢åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
    """
    meta_data = load_cache_meta()
    
    if page_name is None:
        # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
        for cache_key in list(meta_data.keys()):
            cache_file = get_cache_file_path(cache_key)
            if os.path.exists(cache_file):
                os.remove(cache_file)
        meta_data = {}
    else:
        # æ¸…é™¤æŒ‡å®šé¡µé¢çš„ç¼“å­˜
        to_remove = []
        for cache_key, cache_info in meta_data.items():
            if cache_info.get('page_name') == page_name:
                cache_file = get_cache_file_path(cache_key)
                if os.path.exists(cache_file):
                    os.remove(cache_file)
                to_remove.append(cache_key)
        
        for cache_key in to_remove:
            del meta_data[cache_key]
    
    save_cache_meta(meta_data)
    print(f"ç¼“å­˜å·²æ¸…é™¤: {page_name if page_name else 'æ‰€æœ‰é¡µé¢'}")

def get_cache_info():
    """
    è·å–ç¼“å­˜ä¿¡æ¯
    
    Returns:
        dict: ç¼“å­˜ä¿¡æ¯
    """
    meta_data = load_cache_meta()
    
    cache_info = {
        'total_caches': len(meta_data),
        'pages': {},
        'total_size': 0,
        'oldest_cache': None,
        'newest_cache': None
    }
    
    for cache_key, cache_data in meta_data.items():
        page_name = cache_data.get('page_name', 'unknown')
        created_at = datetime.fromisoformat(cache_data.get('created_at', datetime.now().isoformat()))
        file_size = cache_data.get('file_size', 0)
        
        if page_name not in cache_info['pages']:
            cache_info['pages'][page_name] = {
                'count': 0,
                'size': 0,
                'oldest': created_at,
                'newest': created_at
            }
        
        cache_info['pages'][page_name]['count'] += 1
        cache_info['pages'][page_name]['size'] += file_size
        cache_info['pages'][page_name]['oldest'] = min(cache_info['pages'][page_name]['oldest'], created_at)
        cache_info['pages'][page_name]['newest'] = max(cache_info['pages'][page_name]['newest'], created_at)
        
        cache_info['total_size'] += file_size
        
        if cache_info['oldest_cache'] is None or created_at < cache_info['oldest_cache']:
            cache_info['oldest_cache'] = created_at
        
        if cache_info['newest_cache'] is None or created_at > cache_info['newest_cache']:
            cache_info['newest_cache'] = created_at
    
    return cache_info

def format_file_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.1f} KB"
    else:
        return f"{size_bytes / (1024 * 1024):.1f} MB"

def render_cache_management_ui():
    """æ¸²æŸ“ç¼“å­˜ç®¡ç†ç•Œé¢"""
    st.subheader("ğŸ“Š ç¼“å­˜ç®¡ç†")
    
    cache_info = get_cache_info()
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æ€»ç¼“å­˜æ•°", cache_info['total_caches'])
    
    with col2:
        st.metric("æ€»å¤§å°", format_file_size(cache_info['total_size']))
    
    with col3:
        if cache_info['oldest_cache']:
            st.metric("æœ€æ—§ç¼“å­˜", cache_info['oldest_cache'].strftime('%m-%d %H:%M'))
        else:
            st.metric("æœ€æ—§ç¼“å­˜", "æ— ")
    
    with col4:
        if cache_info['newest_cache']:
            st.metric("æœ€æ–°ç¼“å­˜", cache_info['newest_cache'].strftime('%m-%d %H:%M'))
        else:
            st.metric("æœ€æ–°ç¼“å­˜", "æ— ")
    
    # æ˜¾ç¤ºå„é¡µé¢ç¼“å­˜è¯¦æƒ…
    if cache_info['pages']:
        st.subheader("ğŸ“‹ é¡µé¢ç¼“å­˜è¯¦æƒ…")
        
        for page_name, page_info in cache_info['pages'].items():
            with st.expander(f"{page_name} ({page_info['count']}ä¸ªç¼“å­˜)"):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("ç¼“å­˜æ•°é‡", page_info['count'])
                
                with col2:
                    st.metric("æ€»å¤§å°", format_file_size(page_info['size']))
                
                with col3:
                    st.metric("æœ€æ–°ç¼“å­˜", page_info['newest'].strftime('%m-%d %H:%M'))
                
                # æ¸…é™¤è¯¥é¡µé¢ç¼“å­˜çš„æŒ‰é’®
                if st.button(f"ğŸ—‘ï¸ æ¸…é™¤ {page_name} ç¼“å­˜", key=f"clear_{page_name}"):
                    clear_page_cache(page_name)
                    st.success(f"å·²æ¸…é™¤ {page_name} çš„ç¼“å­˜")
                    st.rerun()
    
    # å…¨å±€æ“ä½œ
    st.subheader("ğŸ”§ å…¨å±€æ“ä½œ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰ç¼“å­˜", type="secondary"):
            clear_page_cache()
            st.success("å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜")
            st.rerun()
    
    with col2:
        if st.button("ğŸ”„ åˆ·æ–°ç¼“å­˜ä¿¡æ¯", type="secondary"):
            st.rerun()
    
    # ç¼“å­˜è¯´æ˜
    st.info("""
    **ç¼“å­˜è¯´æ˜ï¼š**
    - æ‰€æœ‰é¡µé¢è®¡ç®—ç»“æœé»˜è®¤ç¼“å­˜24å°æ—¶
    - å‚æ•°å˜åŒ–æ—¶ä¼šè‡ªåŠ¨é‡æ–°è®¡ç®—
    - ç¼“å­˜è¿‡æœŸåä¼šåœ¨ä¸‹æ¬¡è®¿é—®æ—¶è‡ªåŠ¨åˆ·æ–°
    - å¯ä»¥æ‰‹åŠ¨æ¸…é™¤ç‰¹å®šé¡µé¢æˆ–æ‰€æœ‰ç¼“å­˜
    """)

def get_page_cache_key(page_name, **params):
    """
    è·å–é¡µé¢ç¼“å­˜é”®çš„ä¾¿æ·å‡½æ•°
    
    Args:
        page_name: é¡µé¢åç§°
        **params: é¡µé¢å‚æ•°
    
    Returns:
        cache_key: ç¼“å­˜é”®
    """
    return get_cache_key(page_name, params)

def cache_page_result(page_name, result, **params):
    """
    ç¼“å­˜é¡µé¢ç»“æœçš„ä¾¿æ·å‡½æ•°
    
    Args:
        page_name: é¡µé¢åç§°
        result: è®¡ç®—ç»“æœ
        **params: é¡µé¢å‚æ•°
    """
    cache_key = get_cache_key(page_name, params)
    save_to_cache(cache_key, result, page_name, params)

def get_cached_page_result(page_name, **params):
    """
    è·å–é¡µé¢ç¼“å­˜ç»“æœçš„ä¾¿æ·å‡½æ•°
    
    Args:
        page_name: é¡µé¢åç§°
        **params: é¡µé¢å‚æ•°
    
    Returns:
        result: ç¼“å­˜çš„ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨æˆ–æ— æ•ˆåˆ™è¿”å›None
    """
    cache_key = get_cache_key(page_name, params)
    
    if is_cache_valid(cache_key):
        return load_from_cache(cache_key)
    
    return None
