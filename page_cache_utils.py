#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡µé¢è®¡ç®—ç»“æœç¼“å­˜å·¥å…·æ¨¡å—
æ”¯æŒæŒ‰é¡µé¢å’Œå‚æ•°ç¼“å­˜è®¡ç®—ç»“æœï¼Œé»˜è®¤ç¼“å­˜ä¸€å¤©
"""

import streamlit as st
import json
import os
import hashlib
from datetime import datetime, timedelta
import pandas as pd

# ç¼“å­˜é…ç½®
CACHE_DIR = "page_cache"
CACHE_DURATION_HOURS = 24  # ç¼“å­˜24å°æ—¶
CACHE_META_FILE = "cache_meta.json"

def convert_to_json_serializable(data):
    """
    å°†æ•°æ®è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
    
    Args:
        data: è¦è½¬æ¢çš„æ•°æ®
    
    Returns:
        json_data: JSONå¯åºåˆ—åŒ–çš„æ•°æ®
    """
    if isinstance(data, (list, tuple)):
        return [convert_to_json_serializable(item) for item in data]
    elif isinstance(data, dict):
        return {key: convert_to_json_serializable(value) for key, value in data.items()}
    elif hasattr(data, 'to_dict'):  # pandas DataFrame
        return data.to_dict('records')
    elif hasattr(data, 'tolist'):  # numpy array
        return data.tolist()
    elif hasattr(data, 'item'):  # numpy scalar
        return data.item()
    elif isinstance(data, (int, float, str, bool, type(None))):
        return data
    else:
        # å¯¹äºå…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return str(data)

def ensure_cache_dir():
    """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)

def get_cache_key(page_name, params):
    """
    ç”Ÿæˆç¼“å­˜é”®
    
    Args:
        page_name: é¡µé¢åç§°
        params: å‚æ•°å­—å…¸
    
    Returns:
        cache_key: ç¼“å­˜é”®
    """
    # å°†å‚æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ’åºï¼Œç¡®ä¿ç›¸åŒå‚æ•°ç”Ÿæˆç›¸åŒé”®
    params_str = json.dumps(params, sort_keys=True, ensure_ascii=False)
    # ä½¿ç”¨é¡µé¢åç§°å’Œå‚æ•°ç”Ÿæˆå“ˆå¸Œé”®
    cache_key = f"{page_name}_{hashlib.md5(params_str.encode('utf-8')).hexdigest()}"
    return cache_key

def get_cache_file_path(cache_key):
    """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    ensure_cache_dir()
    return os.path.join(CACHE_DIR, f"{cache_key}.json")

def get_cache_meta_file_path():
    """è·å–ç¼“å­˜å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
    ensure_cache_dir()
    return os.path.join(CACHE_DIR, CACHE_META_FILE)

def load_cache_meta():
    """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
    meta_file = get_cache_meta_file_path()
    if os.path.exists(meta_file):
        try:
            with open(meta_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_cache_meta(meta_data):
    """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
    meta_file = get_cache_meta_file_path()
    try:
        with open(meta_file, 'w', encoding='utf-8') as f:
            json.dump(meta_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ä¿å­˜ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")

def is_cache_valid(cache_key):
    """
    æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    
    Args:
        cache_key: ç¼“å­˜é”®
    
    Returns:
        bool: ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    """
    meta_data = load_cache_meta()
    
    if cache_key not in meta_data:
        return False
    
    cache_info = meta_data[cache_key]
    cache_time = datetime.fromisoformat(cache_info['created_at'])
    current_time = datetime.now()
    
    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ç¼“å­˜æ—¶é—´
    if current_time - cache_time > timedelta(hours=CACHE_DURATION_HOURS):
        return False
    
    # æ£€æŸ¥æ˜¯å¦è·¨è¿‡äº†0:00ï¼ˆå¼ºåˆ¶åˆ·æ–°ï¼‰
    cache_date = cache_time.date()
    current_date = current_time.date()
    if cache_date != current_date:
        return False
    
    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    cache_file = get_cache_file_path(cache_key)
    if not os.path.exists(cache_file):
        return False
    
    return True

def save_to_cache(cache_key, data, page_name, params):
    """
    ä¿å­˜æ•°æ®åˆ°ç¼“å­˜
    
    Args:
        cache_key: ç¼“å­˜é”®
        data: è¦ç¼“å­˜çš„æ•°æ®
        page_name: é¡µé¢åç§°
        params: å‚æ•°å­—å…¸
    """
    try:
        # è½¬æ¢æ•°æ®ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
        json_data = convert_to_json_serializable(data)
        
        # ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
        cache_file = get_cache_file_path(cache_key)
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # æ›´æ–°å…ƒæ•°æ®
        meta_data = load_cache_meta()
        meta_data[cache_key] = {
            'page_name': page_name,
            'params': params,
            'created_at': datetime.now().isoformat(),
            'data_type': type(data).__name__,
            'file_size': os.path.getsize(cache_file)
        }
        save_cache_meta(meta_data)
        
        print(f"ç¼“å­˜å·²ä¿å­˜: {page_name} - {cache_key}")
        
    except Exception as e:
        print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def load_from_cache(cache_key):
    """
    ä»ç¼“å­˜åŠ è½½æ•°æ®
    
    Args:
        cache_key: ç¼“å­˜é”®
    
    Returns:
        data: ç¼“å­˜çš„æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–æ— æ•ˆåˆ™è¿”å›None
    """
    try:
        cache_file = get_cache_file_path(cache_key)
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"ä»ç¼“å­˜åŠ è½½: {cache_key}")
            return data
    except Exception as e:
        print(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
    return None

def get_cached_result(page_name, params, compute_func, *args, **kwargs):
    """
    è·å–ç¼“å­˜ç»“æœï¼Œå¦‚æœç¼“å­˜æ— æ•ˆåˆ™é‡æ–°è®¡ç®—
    
    Args:
        page_name: é¡µé¢åç§°
        params: å‚æ•°å­—å…¸
        compute_func: è®¡ç®—å‡½æ•°
        *args: è®¡ç®—å‡½æ•°çš„å‚æ•°
        **kwargs: è®¡ç®—å‡½æ•°çš„å…³é”®å­—å‚æ•°
    
    Returns:
        result: è®¡ç®—ç»“æœ
    """
    cache_key = get_cache_key(page_name, params)
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    if is_cache_valid(cache_key):
        cached_data = load_from_cache(cache_key)
        if cached_data is not None:
            return cached_data
    
    # ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°è®¡ç®—
    print(f"ç¼“å­˜æ— æ•ˆï¼Œé‡æ–°è®¡ç®—: {page_name}")
    result = compute_func(*args, **kwargs)
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if result is not None:
        save_to_cache(cache_key, result, page_name, params)
    
    return result

def clear_page_cache(page_name=None):
    """
    æ¸…é™¤é¡µé¢ç¼“å­˜
    
    Args:
        page_name: é¡µé¢åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
    """
    meta_data = load_cache_meta()
    
    if page_name is None:
        # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
        for cache_key in list(meta_data.keys()):
            cache_file = get_cache_file_path(cache_key)
            if os.path.exists(cache_file):
                os.remove(cache_file)
        meta_data = {}
    else:
        # æ¸…é™¤æŒ‡å®šé¡µé¢çš„ç¼“å­˜
        to_remove = []
        for cache_key, cache_info in meta_data.items():
            if cache_info.get('page_name') == page_name:
                cache_file = get_cache_file_path(cache_key)
                if os.path.exists(cache_file):
                    os.remove(cache_file)
                to_remove.append(cache_key)
        
        for cache_key in to_remove:
            del meta_data[cache_key]
    
    save_cache_meta(meta_data)
    print(f"ç¼“å­˜å·²æ¸…é™¤: {page_name if page_name else 'æ‰€æœ‰é¡µé¢'}")

def get_cache_info():
    """
    è·å–ç¼“å­˜ä¿¡æ¯
    
    Returns:
        dict: ç¼“å­˜ä¿¡æ¯
    """
    meta_data = load_cache_meta()
    
    cache_info = {
        'total_caches': len(meta_data),
        'pages': {},
        'total_size': 0,
        'oldest_cache': None,
        'newest_cache': None
    }
    
    for cache_key, cache_data in meta_data.items():
        page_name = cache_data.get('page_name', 'unknown')
        created_at = datetime.fromisoformat(cache_data.get('created_at', datetime.now().isoformat()))
        file_size = cache_data.get('file_size', 0)
        
        if page_name not in cache_info['pages']:
            cache_info['pages'][page_name] = {
                'count': 0,
                'size': 0,
                'oldest': created_at,
                'newest': created_at
            }
        
        cache_info['pages'][page_name]['count'] += 1
        cache_info['pages'][page_name]['size'] += file_size
        cache_info['pages'][page_name]['oldest'] = min(cache_info['pages'][page_name]['oldest'], created_at)
        cache_info['pages'][page_name]['newest'] = max(cache_info['pages'][page_name]['newest'], created_at)
        
        cache_info['total_size'] += file_size
        
        if cache_info['oldest_cache'] is None or created_at < cache_info['oldest_cache']:
            cache_info['oldest_cache'] = created_at
        
        if cache_info['newest_cache'] is None or created_at > cache_info['newest_cache']:
            cache_info['newest_cache'] = created_at
    
    return cache_info

def format_file_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.1f} KB"
    else:
        return f"{size_bytes / (1024 * 1024):.1f} MB"

def render_cache_management_ui():
    """æ¸²æŸ“ç¼“å­˜ç®¡ç†ç•Œé¢"""
    st.subheader("ğŸ“Š ç¼“å­˜ç®¡ç†")
    
    cache_info = get_cache_info()
    
    # ç¼“å­˜ç»Ÿè®¡å·²ç§»é™¤ï¼Œä¸æ˜¾ç¤ºç»™ç”¨æˆ·
    
    # å…¨å±€æ“ä½œ
    st.subheader("ğŸ”§ å…¨å±€æ“ä½œ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰ç¼“å­˜", type="secondary"):
            clear_page_cache()
            st.success("å·²æ¸…é™¤æ‰€æœ‰ç¼“å­˜")
            st.rerun()
    
    with col2:
        if st.button("ğŸ”„ åˆ·æ–°ç¼“å­˜ä¿¡æ¯", type="secondary"):
            st.rerun()
    
    # ç¼“å­˜è¯´æ˜
    st.info("""
    **ç¼“å­˜è¯´æ˜ï¼š**
    - æ‰€æœ‰é¡µé¢è®¡ç®—ç»“æœé»˜è®¤ç¼“å­˜24å°æ—¶
    - æ¯å¤©0:00åå¼ºåˆ¶åˆ·æ–°æ‰€æœ‰ç¼“å­˜
    - å‚æ•°å˜åŒ–æ—¶ä¼šè‡ªåŠ¨é‡æ–°è®¡ç®—
    - ç¼“å­˜è¿‡æœŸåä¼šåœ¨ä¸‹æ¬¡è®¿é—®æ—¶è‡ªåŠ¨åˆ·æ–°
    - ä½¿ç”¨JSONæ ¼å¼å­˜å‚¨ï¼Œä¾¿äºæŸ¥çœ‹å’Œè°ƒè¯•
    - å¯ä»¥æ‰‹åŠ¨æ¸…é™¤ç‰¹å®šé¡µé¢æˆ–æ‰€æœ‰ç¼“å­˜
    """)

def get_page_cache_key(page_name, **params):
    """
    è·å–é¡µé¢ç¼“å­˜é”®çš„ä¾¿æ·å‡½æ•°
    
    Args:
        page_name: é¡µé¢åç§°
        **params: é¡µé¢å‚æ•°
    
    Returns:
        cache_key: ç¼“å­˜é”®
    """
    return get_cache_key(page_name, params)

def cache_page_result(page_name, result, **params):
    """
    ç¼“å­˜é¡µé¢ç»“æœçš„ä¾¿æ·å‡½æ•°
    
    Args:
        page_name: é¡µé¢åç§°
        result: è®¡ç®—ç»“æœ
        **params: é¡µé¢å‚æ•°
    """
    cache_key = get_cache_key(page_name, params)
    save_to_cache(cache_key, result, page_name, params)

def get_cached_page_result(page_name, **params):
    """
    è·å–é¡µé¢ç¼“å­˜ç»“æœçš„ä¾¿æ·å‡½æ•°
    
    Args:
        page_name: é¡µé¢åç§°
        **params: é¡µé¢å‚æ•°
    
    Returns:
        result: ç¼“å­˜çš„ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨æˆ–æ— æ•ˆåˆ™è¿”å›None
    """
    cache_key = get_cache_key(page_name, params)
    
    if is_cache_valid(cache_key):
        return load_from_cache(cache_key)
    
    return None
